Bert follows a two-step process in the pretraining phase, it is trained on a large corpus of unlabelled text to learn general language representations.
#### Fine-tuning:
In the fine-tuning phase, BERT is further trained on specific NLP tasks with labeled data to adapt the model for specific downstream tasks

During pretraining, we can stop and record the weight of the model as we are training at regular intervals. And then, we can fine-tune each of those models for our specific tasks. and then compare performance of each of those models.